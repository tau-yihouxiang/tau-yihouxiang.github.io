<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="keywords" content="Tao Hu, CSE, CUHK, Chinese University of Hong Kong">
<meta name="description" content="Tao Hu&#39;s home page">

<link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
<title>Tao Hu's Homepage</title>

<link rel="icon" href="files/taohu.jpg" type="image/jpg">
<link rel="apple-touch-icon" href="files/taohu.jpg">

<script async="" src="./files/analytics.js"></script><script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<script type="text/javascript" src="./files/jquery.min.js"></script></head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Dr. Tao Hu 
						<!-- <font face="verdana"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 胡濤博士</font> -->
					</h1></div>
				<p>
					Senior Research Scientist <br>
					Pico MR division <br>
					Bytedance Inc. USA. <br>
					<br>
					<br>
					Email: <a href="mailto:yihouxiang@gmail.com">yihouxiang AT gmail DOT com</a>

				</p>
				<p>
					<a href="https://scholar.google.com/citations?user=xXUg31EAAAAJ&hl=en&amp;hl=en"><img src="./files/google_scholar_logo.png" height="30px"></a>&nbsp;&nbsp;
					<a href="https://www.linkedin.com/in/tao-hu-a4bb2a147/"><img src="./files/LinkedIn_icon.png" height="30px"></a>&nbsp;&nbsp;
				</p>
				<p></p>
			</td>
			<td>
				<img src="./files/taohu.jpg" border="0" width="150"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography [<a href="./files/Tao_Hu-CV.pdf">CV</a>]</h2>
<!-- <h2>Biography </h2> -->
<p>
	</p><p>
	I am a Senior Research Scientist at ByteDance Inc. (Pico MR division) in the United States. Before that, I was a Research Manager (Post-Doctor) in National University of Singapore (NUS), School of Computing, in which I worked with Prof. Gim Hee Lee </a> in 3D Computer Vision.
	I received the Doctor's Degree at Computer Science &amp; Engineering Department, <a href="https://www.cse.cuhk.edu.hk/en/">The Chinese University of Hong Kong (CUHK)</a>, under the supervision of <a href="http://jiaya.me/">Prof. Jiaya Jia</a>.
  	I obtained the Master degree in Computer Science, <a href="https://www.ucas.ac.cn/"> University of Chinese Academy of Sciences (UCAS)</a> in 2019, supervised by <a href="https://people.ucas.edu.cn/~hgqi">Prof. Honggang Qi</a>. I received the Bachelor degree in Automation Engineering from <a href="https://www.uestc.edu.cn"> University of Electronic Science and Technology of China (UESTC)</a> in 2016.
    </p><p>
	Previously, I founded <a href="https://realityedges.com">RealityEdge</a> which combines Large Model with Interaction for creative work.
	</p><p>
    My current research interests includes primarily in 3D Computer Vision and Graphics, particularly focusing on:
    </p>
    <p>
    <b>Efficient 3D Representation, 3D / 4D Scene Generation, 3D Rendering and Reconstruction</b>.
    </p>
  	<p></p>
<p></p>



<h2>Recent News</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td>Nov, 2024</td> <td> <a>Particle Rendering</a>: The paper proposed to improve reflective scene rendering was accepted by 3DV.</b></td>
		</tr>
		<tr>
			<td>Sep, 2024</td> <td> <a href="./projects/X-Ray/X-Ray.html">X-Ray</a>: [Update] The paper was accepted by NeurIPS-2024 as a <b><span style="color: red; font-weight: bold;">Spotlight</span>!</b></td>
		</tr>
		<tr>
			<td>May, 2024</td> <td> <a href="./projects/X-Ray/X-Ray.html">X-Ray</a>: We released a novel representation for 3D Generation from text or image!</td>
		</tr>
		<tr>
			<td>Match, 2024</td> <td> One paper was accepted in CVPR-2024!</td>
		</tr>
		<tr>
			<td>October, 2023</td> <td><a href="https://g3956.github.io">Ref-NeuS</a>: Our paper was selected as the <span style="color: red; font-weight: bold;">ICCV-2023 Best Paper Candidates </span>!</td>
		</tr>
		<tr>
			<td>July, 2023</td> <td><a href="https://g3956.github.io">Ref-NeuS</a>: One Oral paper was accepted by ICCV-2023 about 3D Object Reconstruction!</td>
		</tr>
		<tr>
			<td>April, 2023</td> <td><a href="https://realityeditor.com.cn/product-retryon.html">ReTryOn</a>: We released the application of Virtual TryOn within 10 seconds!</td>
		</tr>
		<tr>
			<td>March, 2023</td><td><a href="https://github.com/Reality-Editor/Composition-Stable-Diffusion">RealityComposition</a>: We released the source code of compositing images via Stable Diffusion!</td>
		</tr>
		<tr>
			<td>March, 2023</td> <td><a href="http://cvpr2023.thecvf.com">CVPR-2023</a>: Our three papers were accepted by CVPR-2023! </td>
		</tr>
	</tbody>
</table>





<h2> Selected Publications [<a href="https://realityedges.com">Google Scholar</a>]</h2>
<table id="tbPublications" width="100%" style="border-collapse:separate; border-spacing:0px 10px;">
	<tbody>

	<tr>
		<td><img width="300" style="padding: 20px;" src="projects/X-Ray/files/teaser.png"></td>
		<td>
			<div><b>X-Ray: A Sequential 3D Representation for Generation.</b></div>
			<div style="font-size: 15px"><i> <b>Tao Hu</b>, Wenhang Ge, Yuyang Zhao, Gim Hee Lee. </i></div>
			<div>NeurIPS-2024 <b> Spotlight (< 3%) </b></div>
			<div>[<a href="https://tau-yihouxiang.github.io/projects/X-Ray/X-Ray.html"><b>Homepage</b> | <a href="https://arxiv.org/abs/2404.14329"><b>Paper</b> | <a href="https://github.com/tau-yihouxiang/X-Ray"><b>Code</b>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/image_restoration.png"></td>
		<td>
			<div><b>Boosting Image Restoration via Priors from Pre-trained Models.</b></div>
			<div style="font-size: 15px"><i> Xiaogang Xu, Shu Kong, <b>Tao Hu</b>, Zhe Liu, Hujun Bao. </i></div>
			<div>CVPR-2024 <b> Poster </b></div>
			<div>[<a href="https://arxiv.org/abs/2403.06793"><b>Paper</b>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/ref-neus.png"></td>
		<td>
			<div><b>Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection.</b></div>
			<div style="font-size: 15px"><i>Wenhang Ge, <b>Tao Hu</b>, Haoyu Zhao, Shu Liu, Ying-Cong Chen. </i></div>
			<div>ICCV-2023 <b> Oral + Best Paper Candidate</b></div>
			<div>[<a href="https://g3956.github.io"><b>Homepage</b></a> | <a href="https://arxiv.org/pdf/2303.10840.pdf"><b>Paper</b></a>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/trivol.png"></td>
		<td>
			<div><b>TriVol: Point Cloud Rendering via Triple Volumes.</b></div>
			<div style="font-size: 15px"><i><b>Tao Hu</b>, Xiaogang Xu, Ruihang Chu, Jiaya Jia.</i></div>
            <div>CVPR-2023</div>
			<div>[<a href="https://arxiv.org/abs/2303.16485"><b>Paper</b></a>|<a href="https://github.com/dvlab-research/TriVol"><b>Code</b></a>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/point2pix.png"></td>
		<td>
			<div><b>Point2Pix: Photo-Realistic Point Cloud Rendering <br> via Neural Radiance Fields.</b></div>
			<div style="font-size: 15px"><i><b>Tao Hu</b>, Xiaogang Xu, Shu Liu, Jiaya.</i></div>
            <div>CVPR-2023</div>
			<div>[<a href="https://arxiv.org/abs/2303.16482"><b>Paper</b></a>|<a href="https://github.com/dvlab-research/TriVol"><b>Code</b></a>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/NeRFLiX.png"></td>
		<td>
			<div><b>NeRFLix: High-Quality Neural View Synthesis by <br> Learning a Degradation-Driven Inter-viewpoint MiXer.</b></div>
			<div style="font-size: 15px"><i>Zhou Kun, Wenbo Li, Yi Wang, <b>Tao Hu</b>, Nianjuan Jiang, <br> Xiaoguang Han, Jiangbo Lu.</i></div>
            <div>CVPR-2023</div>
			<div>[<a href="https://arxiv.org/abs/2303.06919"><b>Paper</b></a>|<a href="https://redrock303.github.io/nerflix/"><b>Project</b></a>]</div>
		</td>
	</tr>

	<!-- Tao Hu, Shu Liu, Yilun Chen, Tianchen Shen, Jiaya Jia, ”EfficientNeRF: Efficient Neural Radiance Fields.” CVPR-2022. -->

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/efficientnerf.png"></td>
		<td>
			<div><b>EfficientNeRF: Efficient Neural Radiance Fields.</b></div>
			<div style="font-size: 15px"><b>Tao Hu</b>, Shu Liu, Yilun Chen, Tianchen Shen, Jiaya Jia.</i></div>
            <div>CVPR-2022</div>
			<div>[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_EfficientNeRF__Efficient_Neural_Radiance_Fields_CVPR_2022_paper.pdf"><b>Paper</b></a>|<a href="https://github.com/dvlab-research/EfficientNeRF"><b>Code</b></a>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/smr.png"></td>
		<td style="font-size: 16px">
			<div><b>Self-Supervised 3D Mesh Reconstruction from Single Images.</b></div>
			<div><i><b>Tao Hu</b>, Liwei Wang, Xiaogang Xu, Shu Liu, Jiaya Jia.</i></div>
            <div>CVPR-2021</div>
			[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Self-Supervised_3D_Mesh_Reconstruction_From_Single_Images_CVPR_2021_paper.pdf"><b>Paper</b></a>|<a href="https://github.com/dvlab-research/SMR"><b>Code</b></a>]
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/wsdan.png"></td>
		<td style="font-size: 16px">
			<div><b>See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification.</b></div>
			<div><i><b>Tao Hu</b>, Honggang Qi, Qingming Huang, Yu Lu.</i></div>
            <div>arXiv:1901.09891</div>
			[<a href="https://arxiv.org/abs/1901.09891"><b>Paper</b></a>|<a href="https://github.com/tau-yihouxiang/WS_DAN"><b>Code</b></a>]
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/LAP.png"></td>
		<td style="font-size: 16px">
			<div><b>Weakly Supervised Local Attention Network for Fine-Grained Visual Classification.</b></div>
			<div><i><b>Tao Hu</b>, Honggang Qi , Cong Huang , Qingming Huang ,Yan Lu ,Jizheng Xu.</i></div>
            <div>arXiv:1808.02152</div>
			[<a href="https://arxiv.org/abs/1901.09891"><b>Paper</b></a>|<a href="https://github.com/tau-yihouxiang/WS_DAN"><b>Code</b></a>]
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/landmark.png"></td>
		<td style="font-size: 16px">
			<div><b>Facial landmarks detection by self-iterative regression based landmarks-attention network.</b></div>
			<div><i><b>Tao Hu</b>, Honggang Qi, Jizheng Xu, Qingming Huang.</i></div>
            <div>AAAI-2018</div>
			[<a href="https://ojs.aaai.org/index.php/AAAI/article/download/12275/12134"><b>Paper</b></a>]
		</td>
	</tr>

	<!-- UA-DETRAC 2017: Report of AVSS2017 & IWT4S challenge on advanced traffic monitoring -->
	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/uadetrac.jpg"></td>
		<td style="font-size: 16px">
			<div><b>UA-DETRAC 2017: Report of AVSS2017 & IWT4S challenge on advanced traffic monitoring.</b></div>
			<div><i>Siwei Lyu, Ming-Ching Chang, Dawei Du, Longyin Wen, Honggang Qi, Yuezun Li, Yi Wei, Lipeng Ke, <b>Tao Hu</b>, Marco Del Coco..</i></div>
            <div>AVSS-2017</div>
			[<a href="https://detrac-db.rit.albany.edu"><b>Homepage</b></a> | <a href="https://drive.google.com/file/d/1ItzjVz3hgOdefSmnl4hHhpkNLw5_m40F/view"><b>Paper</b></a>]
		</td>
	</tr>

    </tbody>
</table>


<h2> Experiences</h2>
<ul>
	<table width="100%" align="center" border="0" cellpadding="10">
        <tbody><tr>
          <td width="25%" align="center">
            <img src="./files/Pico.png" alt="face" width="60%">
          </td>
          <td width="75%" valign="center">
                <strong>Dec. 2024 - Present </strong>, <b>Pico, Bytedance</b>
          </td>
        </tr>
        </tbody>
    </table>
	<table width="100%" align="center" border="0" cellpadding="10">
        <tbody><tr>
          <td width="25%" align="center">
            <img src="./files/NUS.png" alt="face" width="60%">
          </td>
          <td width="75%" valign="center">
                <strong>Aug. 2023 -  Nov. 2024</strong>, <b>National University of Singapore (NUS)</b>
          </td>
        </tr>
        </tbody>
    </table>

	<table width="100%" align="center" border="0" cellpadding="10">
        <tbody><tr>
          <td width="25%" align="center">
            <img src="./files/CUHK.png" alt="face" width="80%">
          </td>
          <td width="75%" valign="center">
                <strong>Aug. 2019 - Aug. 2023</strong>, <b>The Chinese University of Hong Kong (CUHK)</b><br> Ph.D., Supervisor: <a href="https://jiaya.me">Prof. Jiaya Jia</a>  <br>
          </td>
        </tr>
        </tbody>
    </table>

	<table width="100%" align="center" border="0" cellpadding="10">
        <tbody><tr>
          <td width="25%" align="center">
            <img src="./files/MSRA.png" alt="face" width="80%">
          </td>
          <td width="75%" valign="center">
                <strong>Jan. 2018 - Oct. 2018</strong>, <b>Microsoft Research Asia (MSRA)</b><br> Internship, Mentor: <a href="https://scholar.google.com/citations?user=tYzMRrcAAAAJ&hl=en">Dr. Jizheng Xu</a>, <a href="https://scholar.google.com/citations?user=djk5l-4AAAAJ&hl=en">Dr. Yan Lu</a>  <br>
          </td>
        </tr>
        </tbody>
    </table>


    <!-- <table width="100%" align="center" border="0" cellpadding="10">
        <tbody><tr>
          <td width="25%" align="center">
            <img src="./files/Tencent.png" alt="face" width="80%">
          </td>
          <td width="75%" valign="center">
                <strong>Aug. 2019 - Mar. 2020</strong>, <b> Tencent Co.,Ltd.</b><br> Internship, Supervisor: <a href="http://shuliu.me">Dr. Shu Liu</a> <br>
          </td>
        </tr>
        </tbody>
    </table> -->

</ul>


<h2> Honors and Awards</h2>
<ul>
	<li>
		Postgraduate Scholarship, CUHK, 2019-2023
    </li>
    <li>
		Won the <span style="color: red; font-weight: bold;">Runner-up prize</span> in the JDD pig face recognition Challenge.
    </li>
    <li>
        National Encouragement Scholarship, Beijing, 2013 and 2015
    </li>
</ul>

<h2>Professional Services</h2>

<li>	
	<b>Conference Services:</b><br>
	European Conference on Computer Vision (ECCV’24)<br>
	IEEE Conference on Computer Vision and Pattern Recognition (CVPR’20-24)<br>
	IEEE International Conference on Computer Vision (ICCV’19,21)<br>
	European Conference on Computer Vision (ECCV’20)<br>
	<p style="margin-top:3px"></p>
</li>

<li>
    <b>Journal Reviews:</b><br>
	International Journal of Computer Vision (IJCV)<br>
	PATTERN RECOGNITION (PR)<br>
	<p style="margin-top:3px"></p>		
</li>


<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2021-2022</td><td>Spring</td><td>ENGG5104: Image Processing and Computer Vision</td>
		</tr>
		<tr>
			<td> 2019-2020</td><td>Fall</td><td>CSCI1130: Introduction to Computing Using Java</td>
		</tr>
		<tr>
			<td> 2019-2020</td><td>Spring</td><td>CSCI3310:	Mobile Computing and Applications Development</td>
		</tr>

	</tbody>
</table>

<div id="footer">
	<div id="footer-text"></div>
</div>    
        © Tao Hu | Last updated: Dec. 2024
</div>

</body></html>